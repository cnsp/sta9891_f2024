import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.exceptions import ConvergenceWarning
from scipy.sparse import csr_matrix
from sklearn.utils import parallel_backend
import warnings
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re
import time

# Suppress convergence warnings
warnings.simplefilter('ignore', ConvergenceWarning)

# Download NLTK data
nltk.download('stopwords')

# Preprocessing function
def preprocess(text):
    text = text.decode('utf-8') # Decode bytes to string
    text = re.sub(r'[^a-zA-Z\s]', ' ', text.lower())  # Remove non-letter characters and lowercase
    words = text.split()
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    return ' '.join([stemmer.stem(word) for word in words if word not in stop_words])

# Load IMDb dataset
from tensorflow_datasets import tfds

dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
train_dataset, test_dataset = dataset['train'], dataset['test']

X_train_list = [preprocess(text.numpy().decode('utf-8')) for text, _ in train_dataset]
y_train = [label.numpy() for _, label in train_dataset]
X_test_list = [preprocess(text.numpy().decode('utf-8')) for text, _ in test_dataset]
y_test = [label.numpy() for _, label in test_dataset]

# Vectorization with sparse matrices
vectorizer = CountVectorizer()
X_train = csr_matrix(vectorizer.fit_transform(X_train_list))
X_test = csr_matrix(vectorizer.transform(X_test_list))

print("Train Sample Size:", X_train.shape[0])
print("Test Sample Size:", X_test.shape[0])
print("Vocabulary Size:", len(vectorizer.vocabulary_))

# Function to compute cross-validation errors and fit models
def fit_model_gpu(model, X_train, y_train, X_test, y_test, param_grid):
    train_errors, test_errors, times = [], [], []
    for param in param_grid:
        start_time = time.time()
        model.set_params(C=param)
        with parallel_backend('threading'):
            scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy', n_jobs=-1)
        train_errors.append(1 - np.mean(scores))
        model.fit(X_train, y_train)
        train_error = 1 - accuracy_score(y_train, model.predict(X_train))
        test_error = 1 - accuracy_score(y_test, model.predict(X_test))
        test_errors.append(test_error)
        times.append(time.time() - start_time)
    return train_errors, test_errors, times

# Parameter grids
C_values_ridge = np.logspace(-2, 0, num=5)
C_values_lasso = np.logspace(-1, 1, num=5)
C_values_elastic = np.logspace(-1, 1, num=5)

# Ridge Logistic Regression
ridge_model = LogisticRegression(penalty='l2', solver='saga', max_iter=5000)
ridge_train, ridge_test, ridge_times = fit_model_gpu(ridge_model, X_train, y_train, X_test, y_test, C_values_ridge)

# Lasso Logistic Regression
lasso_model = LogisticRegression(penalty='l1', solver='saga', max_iter=5000)
lasso_train, lasso_test, lasso_times = fit_model_gpu(lasso_model, X_train, y_train, X_test, y_test, C_values_lasso)

# ElasticNet Logistic Regression
elastic_model = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga', max_iter=5000)
elastic_train, elastic_test, elastic_times = fit_model_gpu(elastic_model, X_train, y_train, X_test, y_test, C_values_elastic)

# Plot cross-validation errors
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.plot(C_values_ridge, ridge_train, label='Train Error')
plt.plot(C_values_ridge, ridge_test, label='Test Error')
plt.xscale('log')
plt.title('Ridge Logistic Regression')
plt.xlabel('C')
plt.ylabel('Error')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(C_values_lasso, lasso_train, label='Train Error')
plt.plot(C_values_lasso, lasso_test, label='Test Error')
plt.xscale('log')
plt.title('Lasso Logistic Regression')
plt.xlabel('C')
plt.ylabel('Error')
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(C_values_elastic, elastic_train, label='Train Error')
plt.plot(C_values_elastic, elastic_test, label='Test Error')
plt.xscale('log')
plt.title('ElasticNet Logistic Regression')
plt.xlabel('C')
plt.ylabel('Error')
plt.legend()

plt.tight_layout()
plt.show()

# Original Random Forest Implementation (CPU-based)
rf_model = RandomForestClassifier(n_estimators=300, max_features='sqrt', criterion='entropy', random_state=42)
rf_model.fit(X_train, y_train)
cpu_rf_train_error = 1 - accuracy_score(y_train, rf_model.predict(X_train))
cpu_rf_test_error = 1 - accuracy_score(y_test, rf_model.predict(X_test))

# GPU-Accelerated Random Forest Classifier using cuML
from cuml.ensemble import RandomForestClassifier as cuRF

gpu_rf_model = cuRF(n_estimators=300, max_features=int(np.sqrt(X_train.shape[1])), criterion='entropy')
gpu_rf_model.fit(X_train.toarray(), y_train)  # Convert sparse matrix to dense
gpu_rf_train_error = 1 - accuracy_score(y_train, gpu_rf_model.predict(X_train.toarray()))
gpu_rf_test_error = 1 - accuracy_score(y_test, gpu_rf_model.predict(X_test.toarray()))

# Naive Bayes
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
nb_train_error = 1 - accuracy_score(y_train, nb_model.predict(X_train))
nb_test_error = 1 - accuracy_score(y_test, nb_model.predict(X_test))

# Results Table
results = pd.DataFrame({
    'Model': ['Ridge', 'Lasso', 'ElasticNet', 'CPU Random Forest', 'GPU Random Forest', 'Naive Bayes'],
    'Train Error': [round(min(ridge_train), 2), round(min(lasso_train), 2), round(min(elastic_train), 2), round(cpu_rf_train_error, 2), round(gpu_rf_train_error, 2), round(nb_train_error, 2)],
    'Test Error': [round(min(ridge_test), 2), round(min(lasso_test), 2), round(min(elastic_test), 2), round(cpu_rf_test_error, 2), round(gpu_rf_test_error, 2), round(nb_test_error, 2)]
})

# Display the results DataFrame
from IPython.display import display
display(results)
