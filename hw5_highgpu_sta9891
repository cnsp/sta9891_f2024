import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split
from tensorflow.data import Dataset
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk
import re
from IPython.display import display
import time
from tqdm import tqdm

# Download NLTK resources
nltk.download('stopwords')

# Load IMDb dataset
dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
train_dataset, test_dataset = dataset['train'], dataset['test']

# Start timer for total runtime
start_time = time.time()

# Preprocessing function
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess(text):
    text = text.decode('utf-8')  # Decode bytes to string
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)  # Remove non-letter characters
    text = text.lower()  # Convert to lowercase
    words = text.split()  # Split into words
    words = [stemmer.stem(word) for word in words if word not in stop_words]  # Stemming and stop words removal
    return ' '.join(words)

# Preprocess the datasets
X_train_list = [preprocess(text.numpy()) for text, _ in train_dataset]
y_train = [label.numpy() for _, label in train_dataset]
X_test_list = [preprocess(text.numpy()) for text, _ in test_dataset]
y_test = [label.numpy() for _, label in test_dataset]

# Vectorize text data
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(X_train_list)
X_test = vectorizer.transform(X_test_list)

# Dataset Sizes and Vocabulary
print(f"Training samples: {X_train.shape[0]}, Features: {X_train.shape[1]}")
print(f"Test samples: {X_test.shape[0]}, Features: {X_test.shape[1]}")

# Enable Dynamic GPU Memory Growth
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

# TensorFlow Dataset Caching and Prefetching
train_data = Dataset.from_tensor_slices((X_train.toarray(), y_train)).cache().batch(128).prefetch(buffer_size=tf.data.AUTOTUNE)
test_data = Dataset.from_tensor_slices((X_test.toarray(), y_test)).cache().batch(128).prefetch(buffer_size=tf.data.AUTOTUNE)

# Ridge, Lasso, ElasticNet Logistic Regression Models
models = {
    'Ridge': LogisticRegression(penalty='l2', solver='saga', max_iter=5000),
    'Lasso': LogisticRegression(penalty='l1', solver='saga', max_iter=5000),
    'ElasticNet': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=5000)
}

results = pd.DataFrame(columns=['Model', 'Train Error', 'Test Error'])
cv_scores = {}
for model_name, model in tqdm(models.items(), desc="Training Logistic Regression Models"):
    start_model_time = time.time()
    scores = cross_val_score(model, X_train, y_train, cv=10, scoring='neg_log_loss', n_jobs=-1)
    cv_scores[model_name] = -scores
    model.fit(X_train, y_train)
    train_error = log_loss(y_train, model.predict_proba(X_train))
    test_error = log_loss(y_test, model.predict_proba(X_test))
    results.loc[len(results)] = [model_name, round(train_error, 2), round(test_error, 2)]
    print(f"{model_name} training completed in {time.time() - start_model_time:.2f} seconds.")

# Plot Cross-Validation Curves
plt.figure(figsize=(15, 5))
for i, (model_name, scores) in enumerate(cv_scores.items(), start=1):
    plt.subplot(1, 3, i)
    plt.plot(range(1, len(scores) + 1), scores, marker='o')
    plt.title(f'{model_name} CV Scores')
    plt.xlabel('Fold')
    plt.ylabel('Negative Log Loss')
plt.tight_layout()
plt.show()

# Original Random Forest Model
start_rf_time = time.time()
rf_model = RandomForestClassifier(n_estimators=300, criterion='entropy', max_features='sqrt', random_state=42)
rf_model.fit(X_train, y_train)
rf_train_error = 1 - rf_model.score(X_train, y_train)
rf_test_error = 1 - rf_model.score(X_test, y_test)
results.loc[len(results)] = ['Random Forest (CPU)', round(rf_train_error, 2), round(rf_test_error, 2)]
print(f"Random Forest (CPU) training completed in {time.time() - start_rf_time:.2f} seconds.")

# GPU-Accelerated Random Forest (cuML)
try:
    from cuml.ensemble import RandomForestClassifier as cumlRF
    start_cuml_time = time.time()
    cuml_rf_model = cumlRF(n_estimators=300, max_features=1.0, random_state=42)
    cuml_rf_model.fit(X_train.toarray(), y_train)
    gpu_rf_train_error = 1 - cuml_rf_model.score(X_train.toarray(), y_train)
    gpu_rf_test_error = 1 - cuml_rf_model.score(X_test.toarray(), y_test)
    results.loc[len(results)] = ['Random Forest (GPU)', round(gpu_rf_train_error, 2), round(gpu_rf_test_error, 2)]
    print(f"Random Forest (GPU) training completed in {time.time() - start_cuml_time:.2f} seconds.")
except ImportError:
    print("cuML not available; GPU-accelerated Random Forest skipped.")

# Multinomial Naive Bayes
start_nb_time = time.time()
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
nb_train_error = 1 - nb_model.score(X_train, y_train)
nb_test_error = 1 - nb_model.score(X_test, y_test)
results.loc[len(results)] = ['Naive Bayes', round(nb_train_error, 2), round(nb_test_error, 2)]
print(f"Naive Bayes training completed in {time.time() - start_nb_time:.2f} seconds.")

# TensorFlow Model
start_tf_time = time.time()
def create_gpu_model():
    model = Sequential([
        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.3),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

gpu_model = create_gpu_model()
gpu_model.fit(train_data, epochs=10, validation_data=test_data, verbose=1)

# Evaluate TensorFlow Model
train_loss, train_accuracy = gpu_model.evaluate(train_data, verbose=0)
test_loss, test_accuracy = gpu_model.evaluate(test_data, verbose=0)
results.loc[len(results)] = ['TensorFlow Logistic Regression (GPU)', round(1 - train_accuracy, 2), round(1 - test_accuracy, 2)]
print(f"TensorFlow Logistic Regression (GPU) training completed in {time.time() - start_tf_time:.2f} seconds.")

# End timer for total runtime
print(f"Total execution time: {time.time() - start_time:.2f} seconds.")

# Display Final Results
display(results)
